# 深度学习推荐算法之 DIEN

原论文：**《Deep Interest Evolution Network for Click-Through Rate Prediction》**



## 1. 论文背景

​		无论是电商购买行为、还是视频网站的观看行为，或是新闻应用的阅读行为，特定用户的历史行为都是一个随着时间排序的序列。既然是时间相关的序列，就一定存在或深或浅的前后依赖关系，这样的序列信息对于推荐过程无疑是有价值的。但是之前的所有网络，并没有利用这层序列信息。即使是引入了注意力机制的 AFM 或者 DIN 模型，也仅仅是对不同行为的重要性进行打分，这样的得分是时间无关的，或者说是序列无关的。

​		一个典型的电商用户的行为现象可以说明序列信息的重要性。对于一个综合电商来说，用户的兴趣迁移非常快。例如，上周以为用户再跳远一双篮球鞋，这位用户上周的行为序列都会集中在篮球鞋这个品类的商品上，但是他在完成购买之后，本周他的兴趣可能变成买一个机械键盘。序列信息的重要性在于：

- 它加强了最近行为对下次行为预测的影响。在这个历史中，用户近期购买机械键盘的概率会明显高于再买一次篮球鞋或购买其他商品的概率；

- 序列模型能学到购买趋势信息，在这个历史中，序列模型能够在一定程度上建立起 “篮球鞋” 到 “机械键盘” 的转移概率。如果这个转移概率在全局统计意义上足够高，那么用户在购买篮球鞋的时候，推荐机械键盘也会成为一个不错的选项。直观上，二者的用户群体很有可能是一致的。

  如果放弃序列信息，则模型学习时间和趋势这类信息的能力就不会那么强，推荐模型仍然是基于用户所有购买历史的综合推荐，而不是针对 “下一次购买” 的推荐。显然，从业务角度说，后者才是推荐系统正确的推荐目标。

  

## 2. 模型架构

​		基于引进 “序列” 信息的动机，阿里巴巴对 DIN 模型进行了改进，形成了 DIEN 模型架构，如下图所示。模型仍然采用的是 输入层 + Embedding 层 + 连接层 +   多层全连接神经网络的整体架构。图中彩色部分的 “兴趣进化层“ 被认为是一种用户兴趣的 Embedding 方法，它最终的输出是 $h'(T)$ 这个用户兴趣向量。DIEN 模型的创新点就在于如何构建 ”兴趣进化网络“。



​		如上图所示，兴趣进化网络一共可以分成三层，从上到下依次为：

- **行为序列层（浅绿色部分）：**该层的主要作用是把原始的 id 类行为序列转换成 Embedding 行为序列；

- **兴趣抽取层（米黄色部分）：**其主要作用是通过模拟用户兴趣迁移过程，抽取用户兴趣；

- **兴趣进化层（浅红色部分）：**其主要作用是通过兴趣抽取层基础上加入注意力机制，模拟与当前目标广告相关的兴趣进化过程；

  很容易知道，模型架构的重点和主要创新点就是 兴趣抽取层和兴趣进化层部分 这两个部分了。

#### 2.1 兴趣抽取层

​		兴趣抽取层的基本结构是 GRU 网络。相比于传统的序列模型 RNN 和 LSTM，GRU 解决了 RNN 的梯度消失的问题。而与 LSTM 相比，GRU 使用的参数量更少，训练收敛速度更快，因此成为了 DIEN 序列模型的选择。每个 GRU单元的具体计算形式如下：

<img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210218155901144.png" alt="image-20210218155901144" style="zoom:50%;" />
$$
u_t=\sigma(W^ui_t+U^uh_{t-1}+b^u)\\r_t=\sigma(W^ri_t+U^uh_{t-1}+b^r)\\\tilde{h_t}=tanh(W^hi_t+r_t\odot U^hh_{t-1}+b^h)\\h_t=(1-u_t)\odot h_{t-1}+u_t\odot\tilde{h_t}
$$
​		其中，$W^u,W^r,W^h\in R^{n_H\times n_I}$ 而 $U^u,U^r,U^h\in R^{n_H\times n_H}$，其中 $n_H$ 表示的是隐藏向量的维度，$n_I$ 表示的是输入向量的维度。$i_t$ 表示的是 GRU 单元的输入，$i_t=e_b[t]$ 代表的是用户行为序列中的第 t 个行为对应的 embedding 向量，$h_t$ 代表的是 t 时刻的隐藏状态。

​		但是，作者并没有直接使用原来的 GRU 来提取用户兴趣，而是引入了一个辅助函数来指导用户兴趣的提取。作者认为如果直接使用 GRU 提取用户兴趣，只能得到用户行为之间的依赖关系，不能有效的表示用户的兴趣。因为是用户的兴趣导致了用户的点击，用户的最后一次点击与用户点击之前的兴趣相关性很强，但是直接使用行为序列训练 GRU 的话，只有用户最后一次点击的物品（也就是 label，在这里可以认为是 Target Ad），那么最多就是能够捕捉到用户最后一次点击时的兴趣，而最后一次的兴趣又是和前面点击过的物品在兴趣上是相关的，而前面点击的物品中并没有 target item 进行监督。所以作者提出的辅助损失就是为了让行为序列中的每一个时刻都有一个 target item 进行监督训练，也就是使用下一个行为来监督兴趣状态的学习。

**辅助损失：**

​		首先需要明确的就是辅助损失计算哪两个量的损失。计算的是用户每个时刻的兴趣表示（GRU 每个时刻输出的隐藏状态形成的序列）与当前时刻实际点击的物品（输入向量的 embedding 序列）表示之间的损失，相当于是行为序列中的第 t + 1 个物品与用户第 t 时刻的兴趣表示之间的损失。为什么需要用第 t 时刻的兴趣与第 t + 1时刻的真实点击做损失呢？因为根据计算方式，$h_t$ 包含的所有前 t 次用户行为产生的兴趣，我们要预测的是用户的下一次行为与兴趣之间的关系，所以需要用第 t + 1时刻的真实行为。

<img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210218163742638.png" alt="image-20210218163742638" style="zoom:50%;" />

​		当然，如果只计算用户点击物品与其点击前一次兴趣直接之间的损失，只能认为是正样品之间的损失，那么用户第 t 时刻的兴趣其实还有很多其他的未点击的商品，这些未点击的商品就是负样本，负样本一般通过从用户点击序列中采样得到的，这样一来损失函数中就包含了用户某个时刻的兴趣以及与该时刻兴趣相关的正负物品。所以最终的辅助损失函数可以表示为：
$$
L_{aux}=-\frac{1}{N}(\sum_{i=1}^N\sum_{t}\log\sigma(h^i_t,e^i_b[t+1])+\log(1-\sigma(h^i_t,\hat e^i_b[t+1])))
$$
​		其中，$e^i_b\in R^{T\times n_E}$ 表示用户点击序列，而 $\hat e^i_b\in R^{T\times n_E}$ 表示的是负采样得到的样本序列，$\sigma(x_1,x_2)=\frac{1}{1+\exp(-[x_1,x_2])}$，辅助损失函数会加到最终的 CTR 目标函数一起，得到最终的优化目标，并且通过超参数 $\alpha$ 来平衡点击率与兴趣之间的关系。
$$
L=L_{target}+\alpha L_{aux}
$$
​		其中：
$$
L_{target}=-\frac{1}{N}\sum_{(x,y)\in D}^N(y\log P(x)+(1-y)\log (1-P(x)))
$$
**引入辅助损失的意义：**

- 从兴趣抽取的角度来说，引入辅助 loss 可以更好地帮助 GRU 隐藏状态更好地表示用户兴趣；
- 从训练优化的角度来说，辅助 loss 可以在 GRU 处理长序列时，降低优化难度（避免梯度消失？）；
- 辅助 loss 可以给 embedding 层的学习带来更多的语义信息，学习到 item 对应的更好的 embedding 表示；



#### 2.2 兴趣进化层 

​		将用户的行为序列通过 GRU + 辅助 损失建模之后，对用户行为序列中的兴趣进行了提取并表达成了向量的形式（GRU 每个时刻输出的隐藏状态）。而用户的兴趣会因为外部环境或内部认知随着时间变化，特点如下：

- 兴趣时多样化的，可能发生漂移。兴趣漂移对行为的影响时用户可能在一段时间内对各种书籍感兴趣，而在另一段时间内却需要衣服；

- 虽然兴趣可能会相互影响，但是每一种兴趣都有自己的发展过程，例如书和衣服的发展过程几乎是独立的。而我们只需要关注于 target item 相关的演化过程。

  由于用户兴趣是多样的，但是用户每一种兴趣都有自己的发展过程，即使兴趣发生漂移，我们可以只考虑用户与 target item 相关的兴趣演化过程，这样就不用考虑用户多样化的兴趣的问题了，而如何只获取与 target item 相关的信息，作者使用了与 DIN 模型种提取与 target item 相同的方法，来计算用户历史兴趣与 target item 之间的相似度，即这里也使用了 DIN 中介绍的局部激活单元（也就是下面图中的 Attention 模块）。

  <img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210218180755462.png" alt="image-20210218180755462" style="zoom:70%;" />

  具体的计算公式如下：
  $$
  a_t=\frac{\exp(h_tWe_a)}{\sum_{j=1}^T\exp(h_jWe_a)}
  $$
  ​		其中 $e_a$ 是目标 item 的 embedding 向量，$W\in R^{n_H\times n_A}$ ，$n_A$ 是广告 embedding 向量的维度，注意力分数可以反映广告 $e_a$ 和输入 $h_t$ 之间的关系，关系越紧密，则分数越高。在得到注意力分数之后，就需要再次对注意力序列进行建模得到用户注意力的演化过程，进一步表示用户最终的兴趣向量。此时的序列数据等同于有了一个序列及序列中每个向量的注意力权重，下面就是考虑如何使用这个注意力权重来一起优化序列建模的结果。作者提出了三种注意力结合的 GRU 模块：

  - **AIGRU:** 将注意力分数直接与输入序列进行相乘，也就是权重越大的向量对应的值也越大，即输入向量为：
    $$
    i'_t=h_t*a_t
    $$
    但是这种方式存在弊端，比如我们希望当注意力权重很小的时候，当前的输入不会过多影响上一时刻输出的隐藏向量，但是这种方式显然达不到要求，因为即使注意力分数为 0（此时我们理想的结果是 $h'_{t+1}$ 与 $h'_t$ 相同），最终的隐藏状态也会收到影响。

  - **AGRU: **这种方式直接将注意力分数作为 GRU 模块种，更新门的值，则最终的隐藏状态为：
    $$
    h'_t=(1-a_t)h'_{t-1}+a_t*\tilde{h_t}
    $$
    尽管这种方式能直接利用注意力分数的值，来直接控制隐藏状态的更新，但是却忽略了不同维度之间重要性的差异，因为所有维度都乘以相同的常量。

  - **AUGRU: **将注意力分数作为更新门权重，这样既兼顾了注意力分数很低的时候的状态更新值，也避免了所有维度乘以相同的常数，计算为：
    $$
    \tilde{u_t'}=a_t*u_t\\h_t'=(1-\tilde{u_t'})\odot h'_{t-1}+\tilde{u_t'}\odot \tilde h'_t
    $$
    

## 3. 模型总结



