# 深度学习推荐算法之 AFM

原论文：**《Attentional Factiorization Machien: Learning the Weight of Feature Interactions via Attention Networks》**



## 1. 论文背景

​		传统的 FM 模型对所有的交叉特诊都平等地对待，即每一个交叉特征的权重都是相同的。但是在实际的应用中，不同的特征之间交叉的重要性显然是不一样的，一个直观的业务场景，比如，应用场景是预测一位男性用户是否会购买一款键盘的可能性，那么 “性别=男” 和 “购买历史包含鼠标” 这一交叉特征，很可能比 “性别=男” 和 “年龄=30” 这一交叉特征更加重要，模型应该投入更多注意力在前面的交叉熵，也就是说前面的交叉的权重应该更大，如果 “一视同仁” 地对待所有交叉特征，而不考虑不同特征对结果的影响程度，事实上会消解大量有价值的信息。

​		而 AFM 的提出就是为了解决这个问题，也就是让不同特征的交叉具有不同的权重，于是引入了注意力机制，用来学习不同特征之间交叉的不同权重。（事实上，这也可以看作是从业务角度出发的一次尝试，以后会越来越发现，推荐算法的应用场景与业务联系非常紧密，所有很多推荐模型的设计中都可以看到从业务场景得到灵感的影子）



## 2. AFM 模型的原理和架构

​		AFM 模型的架构如下所示：

<img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210131092744905.png" alt="image-20210131092744905" style="zoom: 50%;" />

​		上图表示的 AFM 模型交叉部分的模型结构，而非交叉部分与 FM 模型没有什么差别。所以图中没有给出。 AFM 最核心的两个点分别是 Pair-Wise Interaction Layer 和 Attention-Based Pooling。前者将输入的非零特征的隐向量两两计算 element-wise product，加入输入特征中的非零向量的数量为 m，那么经过 Pair-Wise Interaction Layer 之后的输出向量数目就是 $\frac{m(m-1)}{2}$ ，再将前面得到的交叉特征向量组输入到 Attention-based Pooling，该 Pooling 层会先计算出每个特征组合的自适应权重（通过 Attention Net）进行计算，通过加权平均的方式将向量组压缩成一个向量，由于最终需要输出的是一个数值，所以还是需要将前一步得到的向量通过另外一个向量将其映射成一个值，得到最终的基于注意力加权的二阶交叉特征的输出。

- **Pair-wise Interaction Layer**

  ​		FM 模型中的交叉方式是，所有非零特征对应的隐向量两两点积再求和，输出的是一个数值：
  $$
  \sum_{i=1}^n\sum_{j=i+1}^n<v_i,v_j>x_ix_j
  $$
  ​		AFM 二阶交叉方式是，所有的非零特征对应的隐向量两两元素点积，然后再向量求和，输出还是一个向量。
  $$
  \sum_{i=1}^n\sum_{j=i+1}^n(v_i\odot v_j)x_ix_j
  $$
  ​		从上图中可以看出，作者对数值特征也对应了一个隐向量，不同的数值乘以对应的隐向量就可以得到不同的隐向量，相对于onehot编码的特征乘以1还是其本身(并没有什么变化)，其实就是为了将公式进行统一。虽然论文中给出了对数值特征定义隐向量，但是在作者的代码中并没有发现有对数值特征进行embedding的过程([原论文代码链接](https://github.com/hexiangnan/attentional_factorization_machine/blob/master/code/AFM.py)）具体原因不详。

  ​		按照论文的意思，特征 embedding 可以表示为 :$\varepsilon={v_ix_i}$，经过 Pair-wise Interaction Layer 可以得到输出是：
  $$
  f_{PI}(\epsilon)=\{(v_i\odot v_j)x_ix_j\}_{i,j\in R_x}
  $$
  ​		$R_x$ 表示的是有效特征集合。此时的 $f_{PI}(\epsilon)$ 表示的是一个向量集合，所以需要先将这些向量聚合成一个向量，然后再转换为一个数值：
  $$
  \hat y=p^T\sum_{(i,j)\in R_x}(v_i\odot v_j)x_ix_j +b
  $$
  ​		上式中的求和部分就是将向量集合聚合成一个维度与隐向量维度相同的向量，通过向量$p$再将其转换成一个数值，b表示的是偏置。

  从开始介绍Pair-wise Interaction Layer到现在解决的一个问题是，如何将使用哈达玛积得到的交叉特征转换成一个最终输出需要的数值，到目前为止交叉特征之间的注意力权重还没有出现。在没有详细介绍注意力之前先感性的认识一下如果现在已经有了每个交叉特征的注意力权重，那么交叉特征的输出可以表示为：
  $$
  \hat{y} = p^T \sum_{(i,j)\in R_x}\alpha_{ij}(v_i \odot v_j) x_ix_j + b
  $$
  就是在交叉特征得到的新向量前面乘以一个注意力权重$\alpha_{ij}$, 那么这个注意力权重如何计算得到呢？

- **Attention-based Pooling**

  ​		对于神经网络注意力相关的基础知识大家可以去看一下邱锡鹏老师的《神经网络与深度学习》第8章注意力机制与外部记忆。这里简单的叙述一下使用MLP实现注意力机制的计算。假设现在有n个交叉特征(假如维度是k)，将nxk的数据输入到一个kx1的全连接网络中，输出的张量维度为nx1，使用softmax函数将nx1的向量的每个维度进行归一化，得到一个新的nx1的向量，这个向量所有维度加起来的和为1，每个维度上的值就可以表示原nxk数据每一行(即1xk的数据)的权重。用公式表示为：
  $$
  \alpha_{ij}' = h^T ReLU(W(v_i \odot v_j)x_ix_j + b)
  $$
  使用softmax归一化可得：
  $$
  \alpha_{ij} = \frac{exp(\alpha_{ij}')}{\sum_{(i,j)\in R_x}exp(\alpha_{ij}')}
  $$
  这样就得到了AFM二阶交叉部分的注意力权重，如果将AFM的一阶项写在一起，AFM模型用公式表示为：
  $$
  \hat{y}_{afm}(x) = w_0+\sum_{i=1}^nw_ix_i+p^T \sum_{(i,j)\in R_x}\alpha_{ij}(v_i \odot v_j) x_ix_j + b
  $$

- **AFM 模型的训练**

  ​		AFM 最终的模型公式可以看出与 FM 模型的公式非常相似，所以也可以和 FM 一样用于 不同的任务，例如分类任务、回归任务，以及排序任务，不同任务的损失函数不一样。AFM 也有防止过拟合的处理：

  - 在 Pair-Wise Interaction Layer 层的输出结果上使用 Dropout 防止过拟合，因为并不知道是不是所有的特征组合对结果的影响都是有用的，所以随机去除一些交叉特征，让剩下的特征去自适应得学习可以更好地防止过拟合；

  - 对 Attention-based Pooling 层中的权重矩阵 W 使用 $L_2$ 正则化，作者没有在这一层使用 dropout 的原因是发现同时在交叉层和注意力层加 dropout 会使得模型训练不稳定，并且性能还会下降。

    加上正则参数之后的回归任务的损失函数可表示为 ：
    $$
    L=\sum_{x\in T}(\hat y_{afm}(x)-y(x))^2+\lambda ||W||^2
    $$

## 3. 模型总结

​		AFM 是研究人员从改进模型结构的角度出发，进行的一次有益的尝试，它与具体的应用场景无关，但是阿里巴巴在深度学习推荐模型中引入注意力机制，则是一次基于业务观察的模型改进。